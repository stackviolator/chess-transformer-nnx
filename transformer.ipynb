{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import einops\n",
    "from flax import nnx\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from tokenizer.tokenizer import ChessTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jax Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    debug: bool = True\n",
    "    d_model: int = 768\n",
    "    d_vocab: int = 1882\n",
    "    d_head: int = 64\n",
    "    n_layers: int = 12\n",
    "    n_heads: int = 12\n",
    "    ctx_len: int = 1024\n",
    "    stddev: float = 0.02\n",
    "    d_mlp: int = d_model*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig, key, eps: float = 1e-05):\n",
    "        self.cfg = cfg\n",
    "        self.d_model = self.cfg.d_model\n",
    "        self.w = nnx.Param(jax.random.normal(key, (self.d_model))) # [d_model]\n",
    "        self.b = nnx.Param(jnp.zeros(self.d_model,)) # [d_model]\n",
    "        self.eps = eps\n",
    "    \n",
    "    def __call__(self, residual: jax.Array):\n",
    "        # resdiual: [batch x len x d_model]\n",
    "        # Make mean 0 and normalize to have variance 1\n",
    "        y = (residual - jnp.mean(residual, axis=1, keepdims=True)) / (jnp.sqrt(jnp.var(residual) + self.eps))\n",
    "        # Scale with learned weights\n",
    "        y = y * self.w\n",
    "        # Translate with learned bias\n",
    "        y = y + self.b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig, key):\n",
    "        self.cfg = cfg\n",
    "        self.key = key\n",
    "        self.W_E = nnx.Param(jax.random.normal(self.key, (self.cfg.d_vocab, self.cfg.d_model)) * self.cfg.stddev)\n",
    "\n",
    "    def __call__(self, tokens: jnp.ndarray) -> jnp.ndarray:\n",
    "        # tokens: [batch length]\n",
    "        return self.W_E[tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig, key):\n",
    "        self.cfg = cfg\n",
    "        self.key = key\n",
    "        self.W_pos = nnx.Param(jax.random.normal(self.key, (cfg.ctx_len, cfg.d_model)) * self.cfg.stddev)\n",
    "\n",
    "    def __call__(self, tokens: jnp.ndarray) -> jnp.ndarray:\n",
    "        # tokens: [batch length]\n",
    "        batch, length = tokens.shape\n",
    "        return einops.repeat(self.W_pos[:length], 'length d_model -> batch length d_model', batch=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig, key):\n",
    "        self.cfg = cfg\n",
    "        self.key = key\n",
    "        self.W_Q = nnx.Param(jax.random.normal(self.key, (cfg.n_heads, cfg.d_model, cfg.d_head))) # [num_heads, d_model, d_head]\n",
    "        self.W_K = nnx.Param(jax.random.normal(self.key, (cfg.n_heads, cfg.d_model, cfg.d_head))) # [num_heads, d_model, d_head]\n",
    "        self.W_V = nnx.Param(jax.random.normal(self.key, (cfg.n_heads, cfg.d_model, cfg.d_head))) # [num_heads, d_model, d_head]\n",
    "        self.W_O = nnx.Param(jax.random.normal(self.key, (cfg.n_heads, cfg.d_head, cfg.d_model))) # [num_heads, d_head, d_model]\n",
    "        self.b_Q = nnx.Param(jnp.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_K = nnx.Param(jnp.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_V = nnx.Param(jnp.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_O = nnx.Param(jnp.zeros((cfg.d_model)))\n",
    "\n",
    "    def __call__(self, normal_pre_resid: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        b = batch\n",
    "        l = length\n",
    "        m = d_model\n",
    "        n = num_heads\n",
    "        h = d_head\n",
    "        q = q_pos\n",
    "        k = k_pos\n",
    "        \"\"\"\n",
    "        # normal_pre_resid: [batch length d_model]\n",
    "        q = jnp.einsum('blm, nmh -> blnh', normal_pre_resid, self.W_Q) + self.b_Q\n",
    "        k = jnp.einsum('blm, nmh -> blnh', normal_pre_resid, self.W_K) + self.b_K\n",
    "        v = jnp.einsum('blm, nmh -> blnh', normal_pre_resid, self.W_V) + self.b_V\n",
    "\n",
    "        attn_scores = jnp.einsum('bqnh, bknh -> bnqk', q, k)\n",
    "        attn_scores = self.apply_casual_mask(attn_scores / self.cfg.d_head ** 0.5)\n",
    "        attn_probs = jax.nn.softmax(attn_scores, axis=-1) # [batch x n_heads x q_pos x k_pos]\n",
    "\n",
    "        # [batch x q_pos x n_heads x d_head]\n",
    "        z = jnp.einsum('bnqk, bknh -> bqnh', attn_probs, v)\n",
    "\n",
    "        out = jnp.einsum('bqnh, nhm -> bqnm', z, self.W_O)\n",
    "        out = jnp.einsum('bqnm -> bqm', out) + self.b_O\n",
    "        return out\n",
    "\n",
    "    def apply_casual_mask(self, attn_scores: jnp.ndarray) -> jnp.ndarray:\n",
    "        # attn_scores: [batch n_heads q_pos k_pos]\n",
    "        mask = jnp.triu(attn_scores).astype(bool)\n",
    "        masked_attn_scores = jnp.where(mask,jax.lax.broadcast(-jnp.inf, attn_scores.shape), attn_scores)\n",
    "        \n",
    "        return masked_attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig, key):\n",
    "        self.cfg = cfg\n",
    "        self.key = key\n",
    "        self.W_in = nnx.Param(jax.random.normal(self.key, (cfg.d_model, cfg.d_mlp))) # [d_model, d_mlp]\n",
    "        self.W_out = nnx.Param(jax.random.normal(self.key, (cfg.d_mlp, cfg.d_model))) # [d_mlp, d_model]\n",
    "        self.b_in = nnx.Param(jnp.zeros((cfg.d_mlp)))\n",
    "        self.b_out = nnx.Param(jnp.zeros((cfg.d_model)))\n",
    "\n",
    "    def __call__(self, normal_resid_mid: jnp.ndarray) -> jnp.ndarray:\n",
    "        # normal_resid_mid [batch x length x d_model]\n",
    "        \"\"\"\n",
    "        b = batch\n",
    "        l = length\n",
    "        m = d_model\n",
    "        p = d_mlp\n",
    "        \"\"\"\n",
    "        out = jnp.einsum('blm, mp -> blp', normal_resid_mid, self.W_in) + self.b_in\n",
    "        out = jax.nn.gelu(out)\n",
    "        out = jnp.einsum('blp, pm -> blm', out, self.W_out) + self.b_out\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig, key):\n",
    "        self.cfg = cfg\n",
    "        self.key = key\n",
    "        self.ln1 = LayerNorm(self.cfg, self.key)\n",
    "        self.ln2 = LayerNorm(self.cfg, self.key)\n",
    "        self.attn = Attention(self.cfg, self.key)\n",
    "        self.mlp = MLP(self.cfg, self.key)\n",
    "\n",
    "    def __call__(self, resid_pre: jnp.ndarray) -> jnp.ndarray:\n",
    "        resid_mid = self.attn(self.ln1(resid_pre))\n",
    "        resid_post = self.mlp(self.ln2(resid_pre))\n",
    "        return(resid_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembed(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig, key):\n",
    "        self.cfg = cfg\n",
    "        self.key = key\n",
    "        self.W_U = nnx.Param(jax.random.normal(self.key, (cfg.d_model, cfg.d_vocab)))\n",
    "        self.b_U = nnx.Param(jnp.zeros(cfg.d_vocab))\n",
    "\n",
    "    def __call__(self, normal_resid_post: jnp.ndarray) -> jnp.ndarray:\n",
    "        # normal_resid_post: [batch x length x d_model]\n",
    "        \"\"\"\n",
    "        b = batch\n",
    "        l = length\n",
    "        m = d_model\n",
    "        b = d_vocab\n",
    "        \"\"\"\n",
    "        return jnp.einsum('blm, mv -> blv', normal_resid_post, self.W_U) + self.b_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nnx.Module):\n",
    "    def __init__(self, cfg, key):\n",
    "        self.cfg = cfg\n",
    "        self.key = key\n",
    "        self.embed = Embed(self.cfg, self.key)\n",
    "        self.pos_embed = PosEmbed(self.cfg, self.key)\n",
    "        self.blocks = [TransformerBlock(self.cfg, self.key) for _ in range(cfg.n_layers)]\n",
    "        self.ln_final = LayerNorm(self.cfg, self.key)\n",
    "        self.unembed = Unembed(self.cfg, self.key)\n",
    "\n",
    "    def __call__(self, tokens: jnp.ndarray) -> jnp.ndarray:\n",
    "        resid = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        for block in self.blocks:\n",
    "            resid = block(resid)\n",
    "        logits = self.unembed(self.ln_final(resid))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.key(101)\n",
    "cfg = TransformerConfig(\n",
    "    d_model=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_float_test(cls, key, shape):\n",
    "    random_input = jax.random.uniform(key, (shape))\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = cls(random_input)\n",
    "    if isinstance(output, tuple): output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "def rand_int_test(cls, key, shape):\n",
    "    random_input = jax.random.randint(key, (shape), 100, 1000)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = cls(random_input)\n",
    "    if isinstance(output, tuple): output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4, 64)\n",
      "Output shape: (2, 4, 64) \n",
      "\n",
      "Input shape: (2, 128)\n",
      "Output shape: (2, 128, 64) \n",
      "\n",
      "Input shape: (2, 128)\n",
      "Output shape: (2, 128, 64) \n",
      "\n",
      "Input shape: (2, 128, 64)\n",
      "Output shape: (2, 128, 64) \n",
      "\n",
      "Input shape: (2, 128, 64)\n",
      "Output shape: (2, 128, 64) \n",
      "\n",
      "Input shape: (2, 128, 64)\n",
      "Output shape: (2, 128, 64) \n",
      "\n",
      "Input shape: (2, 128, 64)\n",
      "Output shape: (2, 128, 64) \n",
      "\n",
      "Input shape: (2, 128)\n",
      "Output shape: (2, 128, 64) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LayerNorm test\n",
    "ln = LayerNorm(cfg, key)\n",
    "rand_int_test(ln, key, (2, 4, cfg.d_model))\n",
    "\n",
    "# Embed test\n",
    "emb = Embed(cfg, key)\n",
    "rand_int_test(emb, key, (2, 128))\n",
    "\n",
    "# PosEmbed test\n",
    "pos = PosEmbed(cfg, key)\n",
    "rand_int_test(emb, key, (2, 128))\n",
    "\n",
    "# Attention test\n",
    "attn = Attention(cfg, key)\n",
    "rand_float_test(attn, key, (2, 128, cfg.d_model))\n",
    "\n",
    "# MLP test\n",
    "mlp = MLP(cfg, key)\n",
    "rand_float_test(attn, key, (2, 128, cfg.d_model))\n",
    "\n",
    "# TransformerBlock test\n",
    "tb = TransformerBlock(cfg, key)\n",
    "rand_float_test(attn, key, (2, 128, cfg.d_model))\n",
    "\n",
    "# Unembed test\n",
    "un = Unembed(cfg, key)\n",
    "rand_float_test(attn, key, (2, 128, cfg.d_model))\n",
    "\n",
    "# Transformer test\n",
    "t = Transformer(cfg, key)\n",
    "rand_int_test(emb, key, (2, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ChessTokenizer()\n",
    "tokenizer.load_tokenizer(\"./tokenizer/vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 110, 1882)\n",
      "433\n"
     ]
    }
   ],
   "source": [
    "test_game = [\"<|startofgame|>\", \"e2e4\", \"c7c5\", \"g1f3\", \"d7d6\", \"f1b5\", \"c8d7\", \"d1e2\", \"g8f6\", \"b2b3\", \"e7e6\", \"c1b2\", \"f8e7\", \"e4e5\", \"d6e5\", \"f3e5\", \"e8g8\", \"e1g1\", \"a7a6\", \"e5d7\", \"b8d7\", \"b5d3\", \"b7b5\", \"a2a4\", \"c5c4\", \"b3c4\", \"b5b4\", \"d3e4\", \"a8b8\", \"d2d3\", \"a6a5\", \"b1d2\", \"d7c5\", \"b2e5\", \"b8b6\", \"e4f3\", \"d8d7\", \"d2b3\", \"b6a6\", \"b3c5\", \"e7c5\", \"d3d4\", \"c5e7\", \"f1d1\", \"f8c8\", \"c4c5\", \"a6a7\", \"e2b5\", \"f6d5\", \"b5d7\", \"a7d7\", \"d1d3\", \"f7f6\", \"f3g4\", \"g8f7\", \"e5g3\", \"f6f5\", \"g4h5\", \"g7g6\", \"h5f3\", \"e7f6\", \"g3d6\", \"d7d6\", \"c5d6\", \"c8c2\", \"f3d5\", \"e6d5\", \"a1e1\", \"c2c6\", \"d6d7\", \"c6d6\", \"e1e8\", \"d6d7\", \"e8a8\", \"f6d8\", \"g2g3\", \"f7e6\", \"a8a6\", \"e6f7\", \"g1g2\", \"g6g5\", \"g2f3\", \"d8c7\", \"a6a7\", \"g5g4\", \"f3g2\", \"f7e6\", \"a7b7\", \"e6d6\", \"b7b5\", \"d7e7\", \"g2f1\", \"e7e4\", \"b5b7\", \"h7h5\", \"b7b5\", \"f5f4\", \"f2f3\", \"g4f3\", \"g3f4\", \"e4f4\", \"f1f2\", \"c7d8\", \"b5b7\", \"d8h4\", \"f2f1\", \"f3f2\", \"b7b6\", \"1-0\", \"<|endofgame|>\"]\n",
    "input_ids = tokenizer.encode(test_game)\n",
    "batched_input_ids = jnp.expand_dims(input_ids, 0)\n",
    "\n",
    "transformer = Transformer(cfg, key)\n",
    "\n",
    "logits = transformer(batched_input_ids)\n",
    "\n",
    "# print(logits)\n",
    "print(logits.shape)\n",
    "\n",
    "greedy_pred = jax.nn.softmax(logits[0,-1], axis=-1).argmax()\n",
    "print(f\"Next predicted move is: {tokenizer.decode([greedy_pred])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (minterp)",
   "language": "python",
   "name": "minterp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
