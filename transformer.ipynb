{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "import einops\n",
    "from flax import nnx\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from tokenizer.tokenizer import ChessTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jax Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    debug: bool = True\n",
    "    d_model: int = 768\n",
    "    d_vocab: int = 1882\n",
    "    d_head: int = 64\n",
    "    n_layers: int = 4\n",
    "    n_heads: int = 4\n",
    "    ctx_len: int = 256\n",
    "    stddev: float = 0.02\n",
    "    d_mlp: int = d_model*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig, eps: float = 1e-05):\n",
    "        key = jax.random.PRNGKey(101)\n",
    "        self.cfg = cfg\n",
    "        self.d_model = self.cfg.d_model\n",
    "        self.w = nnx.Param(jax.random.normal(key, (self.d_model))) # [d_model]\n",
    "        self.b = nnx.Param(jnp.zeros(self.d_model,)) # [d_model]\n",
    "        self.eps = eps\n",
    "    \n",
    "    def __call__(self, residual: jax.Array):\n",
    "        # resdiual: [batch x len x d_model]\n",
    "        # Make mean 0 and normalize to have variance 1\n",
    "        y = (residual - jnp.mean(residual, axis=1, keepdims=True)) / (jnp.sqrt(jnp.var(residual) + self.eps))\n",
    "        # Scale with learned weights\n",
    "        y = y * self.w\n",
    "        # Translate with learned bias\n",
    "        y = y + self.b\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig):\n",
    "        key = jax.random.PRNGKey(101)\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nnx.Param(jax.random.normal(key, (self.cfg.d_vocab, self.cfg.d_model)) * self.cfg.stddev)\n",
    "\n",
    "    def __call__(self, tokens: jnp.ndarray) -> jnp.ndarray:\n",
    "        # tokens: [batch length]\n",
    "        return self.W_E[tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig):\n",
    "        key = jax.random.PRNGKey(101)\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nnx.Param(jax.random.normal(key, (cfg.ctx_len, cfg.d_model)) * self.cfg.stddev)\n",
    "\n",
    "    def __call__(self, tokens: jnp.ndarray) -> jnp.ndarray:\n",
    "        # tokens: [batch length]\n",
    "        batch, length = tokens.shape\n",
    "        return einops.repeat(self.W_pos[:length], 'length d_model -> batch length d_model', batch=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig, rngs: nnx.Rngs):\n",
    "        key = rngs.params()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nnx.Param(jax.random.normal(key, (cfg.n_heads, cfg.d_model, cfg.d_head))) # [num_heads, d_model, d_head]\n",
    "        self.W_K = nnx.Param(jax.random.normal(key, (cfg.n_heads, cfg.d_model, cfg.d_head))) # [num_heads, d_model, d_head]\n",
    "        self.W_V = nnx.Param(jax.random.normal(key, (cfg.n_heads, cfg.d_model, cfg.d_head))) # [num_heads, d_model, d_head]\n",
    "        self.W_O = nnx.Param(jax.random.normal(key, (cfg.n_heads, cfg.d_head, cfg.d_model))) # [num_heads, d_head, d_model]\n",
    "        self.b_Q = nnx.Param(jnp.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_K = nnx.Param(jnp.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_V = nnx.Param(jnp.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_O = nnx.Param(jnp.zeros((cfg.d_model)))\n",
    "\n",
    "    def __call__(self, normal_pre_resid: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        b = batch\n",
    "        l = length\n",
    "        m = d_model\n",
    "        n = num_heads\n",
    "        h = d_head\n",
    "        q = q_pos\n",
    "        k = k_pos\n",
    "        \"\"\"\n",
    "        # normal_pre_resid: [batch length d_model]\n",
    "        q = jnp.einsum('blm, nmh -> blnh', normal_pre_resid, self.W_Q.value) + self.b_Q\n",
    "        k = jnp.einsum('blm, nmh -> blnh', normal_pre_resid, self.W_K.value) + self.b_K\n",
    "        v = jnp.einsum('blm, nmh -> blnh', normal_pre_resid, self.W_V.value) + self.b_V\n",
    "\n",
    "        attn_scores = jnp.einsum('bqnh, bknh -> bnqk', q, k)\n",
    "        attn_scores = self.apply_casual_mask(attn_scores / self.cfg.d_head ** 0.5)\n",
    "        attn_probs = jax.nn.softmax(attn_scores, axis=-1) # [batch x n_heads x q_pos x k_pos]\n",
    "\n",
    "        # [batch x q_pos x n_heads x d_head]\n",
    "        z = jnp.einsum('bnqk, bknh -> bqnh', attn_probs, v)\n",
    "\n",
    "        out = jnp.einsum('bqnh, nhm -> bqnm', z, self.W_O.value)\n",
    "        out = jnp.einsum('bqnm -> bqm', out) + self.b_O\n",
    "        return out\n",
    "\n",
    "    def apply_casual_mask(self, attn_scores: jnp.ndarray) -> jnp.ndarray:\n",
    "        # attn_scores: [batch n_heads q_pos k_pos]\n",
    "        mask = jnp.triu(attn_scores).astype(bool)\n",
    "        masked_attn_scores = jnp.where(mask,jax.lax.broadcast(-jnp.inf, attn_scores.shape), attn_scores)\n",
    "        \n",
    "        return masked_attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig):\n",
    "        key = jax.random.PRNGKey(101)\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nnx.Param(jax.random.normal(key, (cfg.d_model, cfg.d_mlp))) # [d_model, d_mlp]\n",
    "        self.W_out = nnx.Param(jax.random.normal(key, (cfg.d_mlp, cfg.d_model))) # [d_mlp, d_model]\n",
    "        self.b_in = nnx.Param(jnp.zeros((cfg.d_mlp)))\n",
    "        self.b_out = nnx.Param(jnp.zeros((cfg.d_model)))\n",
    "\n",
    "    def __call__(self, normal_resid_mid: jnp.ndarray) -> jnp.ndarray:\n",
    "        # normal_resid_mid [batch x length x d_model]\n",
    "        \"\"\"\n",
    "        b = batch\n",
    "        l = length\n",
    "        m = d_model\n",
    "        p = d_mlp\n",
    "        \"\"\"\n",
    "        out = jnp.einsum('blm, mp -> blp', normal_resid_mid, self.W_in.value) + self.b_in\n",
    "        out = jax.nn.gelu(out)\n",
    "        out = jnp.einsum('blp, pm -> blm', out, self.W_out.value) + self.b_out\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig):\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(self.cfg)\n",
    "        self.ln2 = LayerNorm(self.cfg)\n",
    "        self.attn = Attention(self.cfg, rngs=nnx.Rngs(params=0))\n",
    "        self.mlp = MLP(self.cfg)\n",
    "\n",
    "    def __call__(self, resid_pre: jnp.ndarray) -> jnp.ndarray:\n",
    "        resid_mid = self.attn(self.ln1(resid_pre))\n",
    "        resid_post = self.mlp(self.ln2(resid_pre))\n",
    "        return(resid_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembed(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig):\n",
    "        key = jax.random.PRNGKey(101)\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nnx.Param(jax.random.normal(key, (cfg.d_model, cfg.d_vocab)))\n",
    "        self.b_U = nnx.Param(jnp.zeros(cfg.d_vocab))\n",
    "\n",
    "    def __call__(self, normal_resid_post: jnp.ndarray) -> jnp.ndarray:\n",
    "        # normal_resid_post: [batch x length x d_model]\n",
    "        \"\"\"\n",
    "        b = batch\n",
    "        l = length\n",
    "        m = d_model\n",
    "        b = d_vocab\n",
    "        \"\"\"\n",
    "        return jnp.einsum('blm, mv -> blv', normal_resid_post, self.W_U.value) + self.b_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nnx.Module):\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(self.cfg)\n",
    "        self.pos_embed = PosEmbed(self.cfg)\n",
    "        self.blocks = [TransformerBlock(self.cfg) for _ in range(cfg.n_layers)]\n",
    "        self.ln_final = LayerNorm(self.cfg)\n",
    "        self.unembed = Unembed(self.cfg)\n",
    "\n",
    "    def __call__(self, tokens: jnp.ndarray) -> jnp.ndarray:\n",
    "        resid = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        for block in self.blocks:\n",
    "            resid = block(resid)\n",
    "        logits = self.unembed(self.ln_final(resid))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TransformerConfig(\n",
    "    d_model=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_float_test(cls, shape):\n",
    "    random_input = jax.random.uniform(jax.random.PRNGKey(101), (shape))\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = cls(random_input)\n",
    "    if isinstance(output, tuple): output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    random_input = jax.random.randint(jax.random.PRNGKey(101), (shape), 100, 1000)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = cls(random_input)\n",
    "    if isinstance(output, tuple): output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4, 64)\n",
      "Output shape: (2, 4, 64) \n",
      "\n",
      "Input shape: (2, 128)\n",
      "Output shape: (2, 128, 64) \n",
      "\n",
      "Input shape: (2, 128)\n",
      "Output shape: (2, 128, 64) \n",
      "\n",
      "Input shape: (2, 128, 64)\n",
      "Output shape: (2, 128, 64) \n",
      "\n",
      "Input shape: (2, 128, 64)\n",
      "Output shape: (2, 128, 64) \n",
      "\n",
      "Input shape: (2, 128, 64)\n",
      "Output shape: (2, 128, 64) \n",
      "\n",
      "Input shape: (2, 128, 64)\n",
      "Output shape: (2, 128, 64) \n",
      "\n",
      "Input shape: (2, 128)\n",
      "Output shape: (2, 128, 64) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LayerNorm test\n",
    "ln = LayerNorm(cfg)\n",
    "rand_int_test(ln, (2, 4, cfg.d_model))\n",
    "\n",
    "# Embed test\n",
    "emb = Embed(cfg)\n",
    "rand_int_test(emb, (2, 128))\n",
    "\n",
    "# PosEmbed test\n",
    "pos = PosEmbed(cfg)\n",
    "rand_int_test(emb, (2, 128))\n",
    "\n",
    "# Attention test\n",
    "attn = Attention(cfg, rngs=nnx.Rngs(params=0))\n",
    "rand_float_test(attn, (2, 128, cfg.d_model))\n",
    "\n",
    "# MLP test\n",
    "mlp = MLP(cfg)\n",
    "rand_float_test(attn, (2, 128, cfg.d_model))\n",
    "\n",
    "# TransformerBlock test\n",
    "tb = TransformerBlock(cfg)\n",
    "rand_float_test(attn, (2, 128, cfg.d_model))\n",
    "\n",
    "# Unembed test\n",
    "un = Unembed(cfg)\n",
    "rand_float_test(attn, (2, 128, cfg.d_model))\n",
    "\n",
    "# Transformer test\n",
    "t = Transformer(cfg)\n",
    "rand_int_test(emb, (2, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ChessTokenizer()\n",
    "tokenizer.load_tokenizer(\"./tokenizer/vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next predicted move is: f2d1\n"
     ]
    }
   ],
   "source": [
    "test_game = [\"<|startofgame|>\", \"e2e4\", \"c7c5\", \"g1f3\", \"d7d6\", \"f1b5\", \"c8d7\", \"d1e2\", \"g8f6\", \"b2b3\", \"e7e6\", \"c1b2\", \"f8e7\", \"e4e5\", \"d6e5\", \"f3e5\", \"e8g8\", \"e1g1\", \"a7a6\", \"e5d7\", \"b8d7\", \"b5d3\", \"b7b5\", \"a2a4\", \"c5c4\", \"b3c4\", \"b5b4\", \"d3e4\", \"a8b8\", \"d2d3\", \"a6a5\", \"b1d2\", \"d7c5\", \"b2e5\", \"b8b6\", \"e4f3\", \"d8d7\", \"d2b3\", \"b6a6\", \"b3c5\", \"e7c5\", \"d3d4\", \"c5e7\", \"f1d1\", \"f8c8\", \"c4c5\", \"a6a7\", \"e2b5\", \"f6d5\", \"b5d7\", \"a7d7\", \"d1d3\", \"f7f6\", \"f3g4\", \"g8f7\", \"e5g3\", \"f6f5\", \"g4h5\", \"g7g6\", \"h5f3\", \"e7f6\", \"g3d6\", \"d7d6\", \"c5d6\", \"c8c2\", \"f3d5\", \"e6d5\", \"a1e1\", \"c2c6\", \"d6d7\", \"c6d6\", \"e1e8\", \"d6d7\", \"e8a8\", \"f6d8\", \"g2g3\", \"f7e6\", \"a8a6\", \"e6f7\", \"g1g2\", \"g6g5\", \"g2f3\", \"d8c7\", \"a6a7\", \"g5g4\", \"f3g2\", \"f7e6\", \"a7b7\", \"e6d6\", \"b7b5\", \"d7e7\", \"g2f1\", \"e7e4\", \"b5b7\", \"h7h5\", \"b7b5\", \"f5f4\", \"f2f3\", \"g4f3\", \"g3f4\", \"e4f4\", \"f1f2\", \"c7d8\", \"b5b7\", \"d8h4\", \"f2f1\", \"f3f2\", \"b7b6\", \"1-0\", \"<|endofgame|>\"]\n",
    "input_ids = tokenizer.encode(test_game)\n",
    "batched_input_ids = jnp.expand_dims(input_ids, 0)\n",
    "\n",
    "transformer = Transformer(cfg)\n",
    "logits = transformer(batched_input_ids)\n",
    "\n",
    "greedy_pred = jax.nn.softmax(logits[0,-1], axis=-1).argmax()\n",
    "print(f\"Next predicted move is: {tokenizer.decode([greedy_pred])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TraningArgs():\n",
    "    batch_size = 16\n",
    "    epochs = 10\n",
    "    max_steps_per_epoch = 200\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-2\n",
    "    wandb_project_name: str | None = \"ChessTransformer\"\n",
    "    wandb_name: str | None = None\n",
    "\n",
    "args = TraningArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmap\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "# Lots of inspo taken from https://github.com/codyjk/ChessGPT/blob/main/src/chess_model/data/dataset.py -- thank you :^)\n",
    "class GamesDataset(Dataset):\n",
    "    def __init__(self, filename: str, tokenizer, context_length=256):\n",
    "        self.filename = filename\n",
    "        self.tokenizer = tokenizer\n",
    "        self.context_length = context_length\n",
    "        self.line_offsets = []\n",
    "        self.file = open(self.filename, \"r\")\n",
    "\n",
    "        with open(self.filename, 'rb') as f:\n",
    "            mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)\n",
    "            total_size = mm.size()\n",
    "            self.line_offsets.append(0)\n",
    "\n",
    "            with tqdm(\n",
    "                total=total_size, unit=\"B\", unit_scale=True, desc=\"Indexing CSV file\"\n",
    "            ) as pbar:\n",
    "                while mm.readline():\n",
    "                    current_pos = mm.tell()\n",
    "                    self.line_offsets.append(current_pos)\n",
    "                    pbar.update(current_pos - pbar.n)\n",
    "\n",
    "            mm.close()\n",
    "\n",
    "            self.line_offsets.pop()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.line_offsets) - 1\n",
    "\n",
    "    def __getitem__(self, idx) -> dict:\n",
    "        # Add 1 to idx to skip the header\n",
    "        items = {\n",
    "            \"input_ids\": jnp.empty((1, self.context_length),dtype=jnp.int32), # [batch x context_length]\n",
    "            \"labels\": jnp.empty((1, self.context_length), dtype=jnp.int32), # [batch x context_length]\n",
    "            \"is_checkmate\": jnp.empty((1, 1), dtype=jnp.int32),\n",
    "            \"outcome\": jnp.empty((1,3), dtype=jnp.int32),\n",
    "            \"move_mask\": jnp.empty((1, self.context_length), dtype=jnp.int32),\n",
    "        }\n",
    "        if isinstance(idx, int):\n",
    "            idx = [idx]\n",
    "\n",
    "        for i in idx:\n",
    "            \"\"\"\n",
    "            this is a super hack bc im trying to get a line that doesnt exist\n",
    "            on second thought, dont think this is super hacky since we need to + 1 to skip the label row\n",
    "            but this means that if i == len(self.line_offsets) when its + 1'd it will be oob, so decrement just that \n",
    "            \"\"\"\n",
    "            ## TODO fix me plsss\n",
    "            if i == len(self.line_offsets) - 1:\n",
    "                i -= 1\n",
    "            self.file.seek(self.line_offsets[i + 1])\n",
    "            line = self.file.readline().strip()\n",
    "\n",
    "            # Parse the CSV line\n",
    "            row = next(csv.reader([line]))\n",
    "            context, is_checkmate, outcome = row\n",
    "\n",
    "            context = context.split() if context else []\n",
    "            context, last_move = context[:-1], context[-1]\n",
    "            is_checkmate = jnp.array(jnp.expand_dims(float(is_checkmate == \"True\"), axis=0), dtype=jnp.int32)\n",
    "\n",
    "            input_ids = self.tokenizer.encode_and_pad(context, self.context_length)\n",
    "\n",
    "            # Shift context to the left to create labels\n",
    "            # The next move prediction for input_ids[n] is labels[n]\n",
    "            labels = context[1:] + [last_move]\n",
    "            labels = self.tokenizer.encode_and_pad(labels, self.context_length)\n",
    "\n",
    "            # If white won, we want the model to learn from white's moves, not black's.\n",
    "            # Conversely, if black won, we want the model to learn from black's moves.\n",
    "            # For draws, we want the model to learn from both moves.\n",
    "            # We will produce a mask that masks out the moves for the losing player,\n",
    "            # and the model will learn from the remaining moves.\n",
    "            move_mask = jnp.ones(self.context_length, dtype=jnp.int32)\n",
    "\n",
    "            if outcome == \"1-0\":  # White won\n",
    "                # Mask out odd-indexed moves (Black's moves)\n",
    "                move_mask = move_mask.at[1::2].set(0.0)\n",
    "            elif outcome == \"0-1\":  # Black won\n",
    "                # Mask out even-indexed moves (White's moves)\n",
    "                move_mask = move_mask.at[::2].set(0.0)\n",
    "            # For draws (1/2-1/2), keep all moves (mask stays 1)\n",
    "\n",
    "            # If the context is shorter than max_context_length, zero-out that part of the mask\n",
    "            if len(context) < self.context_length:\n",
    "                move_mask = move_mask.at[len(context) :].set(0.0)\n",
    "\n",
    "            # Convert outcome to one-hot encoding (as float)\n",
    "            outcome_label = jnp.zeros(3, dtype=jnp.int32)\n",
    "            if outcome == \"1-0\":\n",
    "                outcome_label = outcome_label.at[0].set(1.0)\n",
    "            elif outcome == \"0-1\":\n",
    "                outcome_label = outcome_label.at[1].set(1.0)\n",
    "            elif outcome == \"1/2-1/2\":\n",
    "                outcome_label = outcome_label.at[2].set(1.0)\n",
    "\n",
    "            items[\"input_ids\"] = jnp.concat((items[\"input_ids\"], jnp.array(jnp.expand_dims(input_ids, axis=0), dtype=jnp.int32))) # expand dims to add empty batch dim\n",
    "            items[\"labels\"] = jnp.concat((items[\"labels\"], jnp.array(jnp.expand_dims(input_ids, axis=0), dtype=jnp.int32)))\n",
    "            items[\"is_checkmate\"] = jnp.concat((items[\"is_checkmate\"], jnp.array(jnp.expand_dims(is_checkmate, axis=1), dtype=jnp.int32)))\n",
    "            items[\"outcome\"] = jnp.concat((items[\"outcome\"], jnp.expand_dims(outcome_label, axis=0)))\n",
    "            items[\"move_mask\"] = jnp.concat((items[\"move_mask\"], jnp.expand_dims(move_mask, axis=0)))\n",
    "\n",
    "        # kinda (very) hacky, removes the first dim since it was the empty dim when initialized\n",
    "        return {\n",
    "        \"input_ids\": items[\"input_ids\"][1:],\n",
    "        \"labels\": items[\"labels\"][1:],\n",
    "        \"is_checkmate\": items[\"is_checkmate\"][1:],\n",
    "        \"outcome\": items[\"outcome\"][1:],\n",
    "        \"move_mask\": items[\"move_mask\"][1:],\n",
    "    }\n",
    "\n",
    "    def __del__(self):\n",
    "        # Close the file when the dataset object is destroyed\n",
    "        if hasattr(self, \"file\"):\n",
    "            self.file.close() \n",
    "\n",
    "    def train_test_split(self, test_size: float = 0.2, random_state: int = 1234) -> dict:\n",
    "        train_indicies, test_indicies = train_test_split(\n",
    "            range(len(self.line_offsets)),\n",
    "            test_size=test_size,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        train_dataset = Subset(self, train_indicies)\n",
    "        test_dataset = Subset(self, test_indicies)\n",
    "        return {\n",
    "            \"train\": train_dataset,\n",
    "            \"test\": test_dataset\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        out = batch[0]\n",
    "        for d in batch[1:]:\n",
    "            for k in d.keys():\n",
    "                out[k] = jnp.concat((out[k], d[k]))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing CSV file: 100%|████████████████████████████████████████████████████████████| 872k/872k [00:00<00:00, 818MB/s]\n",
      "/home/worker/projects/mi-training/minterp/lib/python3.11/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = GamesDataset(\"chess_games.csv\", tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers = 0, collate_fn=GamesDataset.collate_fn)\n",
    "\n",
    "# Test batching works\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    if i_batch == 1:\n",
    "        for d in sample_batched:\n",
    "            pass\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerTrainingArgs():\n",
    "    batch_size = 16\n",
    "    epochs: int = 10\n",
    "    max_steps_per_epoch: int = 200\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-2\n",
    "    wandb_project: str | None = \"ChessTransformer\"\n",
    "    wandb_name: str | None = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f7600d87290>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f75fe862610, raw_cell=\"import optax\n",
      "import wandb\n",
      "\n",
      "class TransformerTraine..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Users/wizard/projects/chess/transformer.ipynb#Y106sZmlsZQ%3D%3D>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f7600d87290>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f72e0f4acd0, execution_count=172 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f75fe862610, raw_cell=\"import optax\n",
      "import wandb\n",
      "\n",
      "class TransformerTraine..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Users/wizard/projects/chess/transformer.ipynb#Y106sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "import wandb\n",
    "\n",
    "class TransformerTrainer:\n",
    "    def __init__(self, args: TransformerTrainingArgs, model: Transformer, train_loader, test_loader):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "        self.optimizer = nnx.Optimizer(self.model, optax.adamw(learning_rate=args.lr, weight_decay=args.weight_decay))\n",
    "        self.step = 0\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "    def training_step(self, batch: dict) -> jnp.ndarray:\n",
    "        def loss_fn(model: Transformer):\n",
    "            y_pred = model(batch[\"input_ids\"])\n",
    "            log_probs = self.get_log_probs(y_pred, batch[\"input_ids\"])\n",
    "            return -log_probs.mean()\n",
    "\n",
    "        loss, grads = nnx.value_and_grad(loss_fn)(self.model)\n",
    "        self.optimizer.update(grads)\n",
    "\n",
    "        self.step += 1\n",
    "        wandb.log({\"train_loss\":loss}, step=self.step)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: dict) -> jnp.ndarray:\n",
    "        tokens = batch[\"input_ids\"]\n",
    "        logits = self.model(tokens)[:,:-1]\n",
    "        pred_tokens = jnp.argmax(logits, axis=-1)\n",
    "        correct = (pred_tokens == tokens[:, 1:]).flatten()\n",
    "\n",
    "        return correct\n",
    "\n",
    "    def train(self):\n",
    "        wandb.init(project=self.args.wandb_project, name=self.args.wandb_name, config=self.args)\n",
    "        accuracy = jnp.nan\n",
    "        total_steps = self.args.epochs * self.args.max_steps_per_epoch\n",
    "\n",
    "        with tqdm(total=total_steps, desc=\"Training Epochs\") as progress_bar:\n",
    "            for epoch in range(self.args.epochs):\n",
    "                for i, batch in enumerate(self.train_loader):\n",
    "                    loss = self.training_step(batch)\n",
    "                    progress_bar.update()\n",
    "                    progress_bar.set_description(f\"Epoch {epoch+1}, loss: {loss:.3f}, accuracy: {accuracy:.2f}\")\n",
    "                    if i >= self.args.max_steps_per_epoch:\n",
    "                        break\n",
    "\n",
    "                correct = jnp.concat([self.validation_step(batch) for batch in self.test_loader])\n",
    "                accuracy = jnp.mean(correct)\n",
    "                wandb.log({\"accuracy\":accuracy}, step=self.step)\n",
    "\n",
    "        wandb.finish()\n",
    "\n",
    "    # TODO write unittest for this PLEASE\n",
    "    def get_log_probs(self, logits: jnp.ndarray, tokens: jnp.ndarray) -> jnp.ndarray:\n",
    "        log_probs = nnx.log_softmax(logits, axis=-1)\n",
    "        sliced_log_probs = log_probs[:, :-1]\n",
    "        next_token_indicies = jnp.expand_dims(tokens[:, 1:], axis=-1).astype(jnp.int32)\n",
    "        log_probs_for_tokens = jnp.take_along_axis(\n",
    "            sliced_log_probs, next_token_indicies, axis=-1\n",
    "        )\n",
    "\n",
    "        return log_probs_for_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f74d5f6c3d0>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f72e7aea8d0, raw_cell=\"dataset = GamesDataset(\"chess_games.csv\", tokenize..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Users/wizard/projects/chess/transformer.ipynb#Y103sZmlsZQ%3D%3D>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing CSV file: 100%|████████████████████████████████████████████████████████████| 872k/872k [00:00<00:00, 797MB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:slofni3u) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>▂▅▇▄█▆▁▄█▃▅▇▄▇▇█▆▁▁▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>2.15735</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vague-rain-3</strong> at: <a href='https://wandb.ai/joshmerrill-none/ChessTransformer/runs/slofni3u' target=\"_blank\">https://wandb.ai/joshmerrill-none/ChessTransformer/runs/slofni3u</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241119_191709-slofni3u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:slofni3u). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095df46983e344fba79ef9f1ef74ff28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669611033285035, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/worker/projects/chess/wandb/run-20241119_192109-05fn7wlh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joshmerrill-none/ChessTransformer/runs/05fn7wlh' target=\"_blank\">dauntless-violet-4</a></strong> to <a href='https://wandb.ai/joshmerrill-none/ChessTransformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joshmerrill-none/ChessTransformer' target=\"_blank\">https://wandb.ai/joshmerrill-none/ChessTransformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joshmerrill-none/ChessTransformer/runs/05fn7wlh' target=\"_blank\">https://wandb.ai/joshmerrill-none/ChessTransformer/runs/05fn7wlh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 1.350, accuracy: nan:   4%|█▌                                      | 76/2000 [02:03<1:00:14,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7146509885787964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, loss: 0.959, accuracy: 0.71:   8%|███                                     | 152/2000 [05:32<36:16,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.715843141078949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, loss: 1.065, accuracy: 0.72:  15%|██████                                  | 304/2000 [12:30<33:36,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7176784873008728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, loss: 1.162, accuracy: 0.72:  19%|███████▌                                | 380/2000 [15:57<29:58,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7187294363975525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, loss: 2.466, accuracy: 0.72:  23%|█████████                               | 456/2000 [19:25<30:52,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7190862894058228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, loss: 1.232, accuracy: 0.72:  27%|██████████▋                             | 532/2000 [22:53<28:51,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7191490530967712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 1.860, accuracy: 0.72:  30%|████████████▏                           | 608/2000 [26:20<26:54,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7189686894416809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, loss: 1.794, accuracy: 0.72:  34%|█████████████▋                          | 684/2000 [29:48<26:54,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7184706330299377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, loss: 1.194, accuracy: 0.72:  38%|██████████████▊                        | 760/2000 [34:45<56:42,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7184706330299377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153224c82bfe496494ec1e2a8176b258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>train_loss</td><td>▆▄█▅▅▄█▆█▅▅▆▅▅▆▅▃▅▄▆▂▄▄▄▄▄▄▆▄▄▅▃▅▃▄▅▄▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.71847</td></tr><tr><td>train_loss</td><td>1.19364</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dauntless-violet-4</strong> at: <a href='https://wandb.ai/joshmerrill-none/ChessTransformer/runs/05fn7wlh' target=\"_blank\">https://wandb.ai/joshmerrill-none/ChessTransformer/runs/05fn7wlh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241119_192109-05fn7wlh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = GamesDataset(\"chess_games.csv\", tokenizer, context_length=256)\n",
    "dataset_dict = dataset.train_test_split(test_size=1000)\n",
    "\n",
    "train_dataset = dataset_dict[\"train\"]\n",
    "test_dataset = dataset_dict[\"test\"]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0, pin_memory=True, collate_fn=GamesDataset.collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0, pin_memory=True, collate_fn=GamesDataset.collate_fn)\n",
    "\n",
    "args = TransformerTrainingArgs(epochs=10, max_steps_per_epoch=200)\n",
    "trainer = TransformerTrainer(args, transformer, train_loader=train_loader, test_loader=test_loader)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (minterp)",
   "language": "python",
   "name": "minterp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
