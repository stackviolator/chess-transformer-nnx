{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import einops\n",
    "from flax import nnx\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import tokenizer/tokneizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jax Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    debug: bool = True\n",
    "    d_model: int = 768\n",
    "    d_vocab: int = 50257\n",
    "    d_head: int = 64\n",
    "    n_layers: int = 12\n",
    "    n_heads: int = 12\n",
    "    ctx_len: int = 1024\n",
    "    stddev: float = 0.02\n",
    "    d_mlp: int = d_model*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig, key, eps: float = 1e-05):\n",
    "        self.cfg = cfg\n",
    "        self.d_model = self.cfg.d_model\n",
    "        self.w = nnx.Param(jax.random.normal(key, (self.d_model))) # [d_model]\n",
    "        self.b = nnx.Param(jnp.zeros(self.d_model,)) # [d_model]\n",
    "        self.eps = eps\n",
    "    \n",
    "    def __call__(self, residual: jax.Array):\n",
    "        # resdiual: [batch x len x d_model]\n",
    "        # Make mean 0 and normalize to have variance 1\n",
    "        y = (residual - jnp.mean(residual, axis=1, keepdims=True)) / (jnp.sqrt(jnp.var(residual) + self.eps))\n",
    "        # Scale with learned weights\n",
    "        y = y * self.w\n",
    "        # Translate with learned bias\n",
    "        y = y + self.b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig, key):\n",
    "        self.cfg = cfg\n",
    "        self.key = key\n",
    "        self.W_E = nnx.Param(jax.random.normal(self.key, (self.cfg.d_vocab, self.cfg.d_model)) * self.cfg.stddev)\n",
    "\n",
    "    def __call__(self, tokens: jnp.ndarray) -> jnp.ndarray:\n",
    "        # tokens: [batch length]\n",
    "        return self.W_E[tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig, key):\n",
    "        self.cfg = cfg\n",
    "        self.key = key\n",
    "        self.W_pos = nnx.Param(jax.random.normal(self.key, (cfg.ctx_len, cfg.d_model)) * self.cfg.stddev)\n",
    "\n",
    "    def __call__(self, tokens: jnp.ndarray) -> jnp.ndarray:\n",
    "        # tokens: [batch length]\n",
    "        batch, length = tokens.shape\n",
    "        return einops.repeat(self.W_pos[:length], 'length d_model -> batch length d_model', batch=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig, key):\n",
    "        self.cfg = cfg\n",
    "        self.key = key\n",
    "        self.W_Q = nnx.Param(jax.random.normal(self.key, (cfg.n_heads, cfg.d_model, cfg.d_head))) # [num_heads, d_model, d_head]\n",
    "        self.W_K = nnx.Param(jax.random.normal(self.key, (cfg.n_heads, cfg.d_model, cfg.d_head))) # [num_heads, d_model, d_head]\n",
    "        self.W_V = nnx.Param(jax.random.normal(self.key, (cfg.n_heads, cfg.d_model, cfg.d_head))) # [num_heads, d_model, d_head]\n",
    "        self.W_O = nnx.Param(jax.random.normal(self.key, (cfg.n_heads, cfg.d_head, cfg.d_model))) # [num_heads, d_head, d_model]\n",
    "        self.b_Q = nnx.Param(jnp.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_K = nnx.Param(jnp.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_V = nnx.Param(jnp.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_O = nnx.Param(jnp.zeros((cfg.d_model)))\n",
    "\n",
    "    def __call__(self, normal_pre_resid: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        b = batch\n",
    "        l = length\n",
    "        m = d_model\n",
    "        n = num_heads\n",
    "        h = d_head\n",
    "        q = q_pos\n",
    "        k = k_pos\n",
    "        \"\"\"\n",
    "        # normal_pre_resid: [batch length d_model]\n",
    "        q = jnp.einsum('blm, nmh -> blnh', normal_pre_resid, self.W_Q) + self.b_Q\n",
    "        k = jnp.einsum('blm, nmh -> blnh', normal_pre_resid, self.W_K) + self.b_K\n",
    "        v = jnp.einsum('blm, nmh -> blnh', normal_pre_resid, self.W_V) + self.b_V\n",
    "\n",
    "        attn_scores = jnp.einsum('bqnh, bknh -> bnqk', q, k)\n",
    "        attn_scores = self.apply_casual_mask(attn_scores / self.cfg.d_head ** 0.5)\n",
    "        attn_probs = jax.nn.softmax(attn_scores, axis=-1) # [batch x n_heads x q_pos x k_pos]\n",
    "\n",
    "        # [batch x q_pos x n_heads x d_head]\n",
    "        z = jnp.einsum('bnqk, bknh -> bqnh', attn_probs, v)\n",
    "\n",
    "        out = jnp.einsum('bqnh, nhm -> bqnm', z, self.W_O)\n",
    "        out = jnp.einsum('bqnm -> bqm', out) + self.b_O\n",
    "        return out\n",
    "\n",
    "    def apply_casual_mask(self, attn_scores: jnp.ndarray) -> jnp.ndarray:\n",
    "        # attn_scores: [batch n_heads q_pos k_pos]\n",
    "        mask = jnp.triu(attn_scores).astype(bool)\n",
    "        masked_attn_scores = jnp.where(mask,jax.lax.broadcast(-jnp.inf, attn_scores.shape), attn_scores)\n",
    "        \n",
    "        return masked_attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig, key):\n",
    "        self.cfg = cfg\n",
    "        self.key = key\n",
    "        self.W_in = nnx.Param(jax.random.normal(self.key, (cfg.d_model, cfg.d_mlp))) # [d_model, d_mlp]\n",
    "        self.W_out = nnx.Param(jax.random.normal(self.key, (cfg.d_mlp, cfg.d_model))) # [d_mlp, d_model]\n",
    "        self.b_in = nnx.Param(jnp.zeros((cfg.d_mlp)))\n",
    "        self.b_out = nnx.Param(jnp.zeros((cfg.d_model)))\n",
    "\n",
    "    def __call__(self, normal_resid_mid: jnp.ndarray) -> jnp.ndarray:\n",
    "        # normal_resid_mid [batch x length x d_model]\n",
    "        \"\"\"\n",
    "        b = batch\n",
    "        l = length\n",
    "        m = d_model\n",
    "        p = d_mlp\n",
    "        \"\"\"\n",
    "        out = jnp.einsum('blm, mp -> blp', normal_resid_mid, self.W_in) + self.b_in\n",
    "        out = jax.nn.gelu(out)\n",
    "        out = jnp.einsum('blp, pm -> blm', out, self.W_out) + self.b_out\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig, key):\n",
    "        self.cfg = cfg\n",
    "        self.key = key\n",
    "        self.ln1 = LayerNorm(self.cfg, self.key)\n",
    "        self.ln2 = LayerNorm(self.cfg, self.key)\n",
    "        self.attn = Attention(self.cfg, self.key)\n",
    "        self.mlp = MLP(self.cfg, self.key)\n",
    "\n",
    "    def __call__(self, resid_pre: jnp.ndarray) -> jnp.ndarray:\n",
    "        resid_mid = self.attn(self.ln1(resid_pre))\n",
    "        resid_post = self.mlp(self.ln2(resid_pre))\n",
    "        return(resid_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembed(nnx.Module):\n",
    "    def __init__(self, cfg: TransformerConfig, key):\n",
    "        self.cfg = cfg\n",
    "        self.key = key\n",
    "        self.W_U = nnx.Param(jax.random.normal(self.key, (cfg.d_model, cfg.d_vocab)))\n",
    "        self.b_U = nnx.Param(jnp.zeros(cfg.d_vocab))\n",
    "\n",
    "    def __call__(self, normal_resid_post: jnp.ndarray) -> jnp.ndarray:\n",
    "        # normal_resid_post: [batch x length x d_model]\n",
    "        \"\"\"\n",
    "        b = batch\n",
    "        l = length\n",
    "        m = d_model\n",
    "        b = d_vocab\n",
    "        \"\"\"\n",
    "        return jnp.einsum('blm, mv -> blv', normal_resid_post, self.W_U) + self.b_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nnx.Module):\n",
    "    def __init__(self, cfg, key):\n",
    "        self.cfg = cfg\n",
    "        self.key = key\n",
    "        self.embed = Embed(self.cfg, self.key)\n",
    "        self.pos_embed = PosEmbed(self.cfg, self.key)\n",
    "        self.blocks = [TransformerBlock(self.cfg, self.key) for _ in range(cfg.n_layers)]\n",
    "        self.ln_final = LayerNorm(self.cfg, self.key)\n",
    "        self.unembed = Unembed(self.cfg, self.key)\n",
    "\n",
    "    def __call__(self, tokens: jnp.ndarray) -> jnp.ndarray:\n",
    "        resid = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        for block in self.blocks:\n",
    "            resid = block(resid)\n",
    "        logits = self.unembed(self.ln_final(resid))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.key(101)\n",
    "cfg = TransformerConfig(\n",
    "    d_model=64,\n",
    "    d_vocab=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_float_test(cls, key, shape):\n",
    "    random_input = jax.random.uniform(key, (shape))\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = cls(random_input)\n",
    "    if isinstance(output, tuple): output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "def rand_int_test(cls, key, shape):\n",
    "    random_input = jax.random.randint(key, (shape), 100, 1000)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = cls(random_input)\n",
    "    if isinstance(output, tuple): output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4, 64)\n",
      "Output shape: (2, 4, 64) \n",
      "\n",
      "Input shape: (2, 128)\n",
      "Output shape: (2, 128, 64) \n",
      "\n",
      "Input shape: (2, 128)\n",
      "Output shape: (2, 128, 64) \n",
      "\n",
      "Input shape: (2, 128, 64)\n",
      "Output shape: (2, 128, 64) \n",
      "\n",
      "Input shape: (2, 128, 64)\n",
      "Output shape: (2, 128, 64) \n",
      "\n",
      "Input shape: (2, 128, 64)\n",
      "Output shape: (2, 128, 64) \n",
      "\n",
      "Input shape: (2, 128, 64)\n",
      "Output shape: (2, 128, 64) \n",
      "\n",
      "Input shape: (2, 128)\n",
      "Output shape: (2, 128, 64) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LayerNorm test\n",
    "ln = LayerNorm(cfg, key)\n",
    "rand_int_test(ln, key, (2, 4, cfg.d_model))\n",
    "\n",
    "# Embed test\n",
    "emb = Embed(cfg, key)\n",
    "rand_int_test(emb, key, (2, 128))\n",
    "\n",
    "# PosEmbed test\n",
    "pos = PosEmbed(cfg, key)\n",
    "rand_int_test(emb, key, (2, 128))\n",
    "\n",
    "# Attention test\n",
    "attn = Attention(cfg, key)\n",
    "rand_float_test(attn, key, (2, 128, cfg.d_model))\n",
    "\n",
    "# MLP test\n",
    "mlp = MLP(cfg, key)\n",
    "rand_float_test(attn, key, (2, 128, cfg.d_model))\n",
    "\n",
    "# TransformerBlock test\n",
    "tb = TransformerBlock(cfg, key)\n",
    "rand_float_test(attn, key, (2, 128, cfg.d_model))\n",
    "\n",
    "# Unembed test\n",
    "un = Unembed(cfg, key)\n",
    "rand_float_test(attn, key, (2, 128, cfg.d_model))\n",
    "\n",
    "# Transformer test\n",
    "t = Transformer(cfg, key)\n",
    "rand_int_test(emb, key, (2, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Ċ', 'This', 'Ġis', 'Ġsample', 'Ġtext', 'Ġthat', 'ĠI', 'Ġam', 'Ġgoing', 'Ġto', 'Ġtoken', 'ize', '.', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "This is sample text that I am going to tokenize.\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(text, return_tensors=\"np\")\n",
    "input_ids, attention_mask = jnp.array(input_ids['input_ids']), jnp.array(input_ids['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 9.9230218e-01  1.5166521e-02 -5.0949659e+00 ...  2.2485504e+00\n",
      "    4.3776274e-02 -2.3710639e+00]\n",
      "  [-4.2430086e+00 -7.9845352e+00 -5.5063744e+00 ... -6.9265957e+00\n",
      "   -7.1119471e+00 -1.6937866e+00]\n",
      "  [-3.8464799e+00  1.7938212e+00  6.7881411e-01 ...  1.6993171e+00\n",
      "    6.0196104e+00 -3.9587855e-01]\n",
      "  ...\n",
      "  [ 8.0927715e+00 -2.7217324e+00 -8.9893293e+00 ... -1.2862199e+01\n",
      "   -1.2313092e+00 -4.1354780e+00]\n",
      "  [-1.7582830e+00 -1.7279179e+01  7.8891530e+00 ... -1.8498978e+01\n",
      "    7.2134991e+00 -4.6433630e+00]\n",
      "  [-2.3629048e+00 -5.6193485e+00 -3.3724360e+00 ...  9.7430378e-01\n",
      "   -5.7956591e+00  8.1657858e+00]]]\n",
      "(1, 14, 1024)\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(cfg, key)\n",
    "\n",
    "logits = transformer(input_ids)\n",
    "\n",
    "print(logits)\n",
    "print(logits.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (minterp)",
   "language": "python",
   "name": "minterp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
